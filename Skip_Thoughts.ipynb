{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_directory = './data/'\n",
    "\n",
    "data_train = pd.read_csv(os.path.join(data_directory, 'train_stories.csv'), header='infer')\n",
    "# print(data_train.columns)\n",
    "\n",
    "data_val = pd.read_csv(os.path.join(data_directory, 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'), header='infer')\n",
    "# print(data_val.columns)\n",
    "# data_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# train_embeddings = np.load('/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo/skip-thoughts-embbedings.npy')\n",
    "validation_embeddings = np.load('/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo/skip-thoughts-embbedings_validation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 6, 4800)\n"
     ]
    }
   ],
   "source": [
    "train_on_validation = True\n",
    "\n",
    "if 'train_embeddings' in locals():\n",
    "    print(train_embeddings.shape)\n",
    "    train_on_validation = False\n",
    "    \n",
    "print(validation_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "v_embeddings = list()\n",
    "v_classes = list()\n",
    "correct_answers = data_val['AnswerRightEnding'].values\n",
    "\n",
    "for i, story_embedding in enumerate(validation_embeddings):\n",
    "    v_embeddings.append(np.append(story_embedding[:4], [story_embedding[4]], axis=0))\n",
    "    v_embeddings.append(np.append(story_embedding[:4], [story_embedding[5]], axis=0))\n",
    "    \n",
    "    if correct_answers[i] == 1:\n",
    "        v_classes.append(0)\n",
    "        v_classes.append(1)\n",
    "    else:\n",
    "        v_classes.append(1)\n",
    "        v_classes.append(0)\n",
    "        \n",
    "v_embeddings = np.array(v_embeddings)\n",
    "v_classes = np.array(v_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2974\n",
      "Test size:  768\n"
     ]
    }
   ],
   "source": [
    "# split to train and test\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "if train_on_validation:\n",
    "    train_size = 0.80\n",
    "\n",
    "    np.random.seed(42)\n",
    "    msk = np.random.rand(len(v_embeddings) // 2) < train_size\n",
    "    new_mask = np.array([[value, value] for value in msk]).reshape(-1)\n",
    "\n",
    "    train_embeddings, train_classes = shuffle(v_embeddings[new_mask], v_classes[new_mask])\n",
    "\n",
    "    test_embeddings, test_classes = v_embeddings[~new_mask], v_classes[~new_mask]\n",
    "    \n",
    "else:\n",
    "    train_embeddings, train_classes = shuffle(train_embeddings), np.zeros(len(train_embeddings))\n",
    "\n",
    "    test_embeddings, test_classes = v_embeddings, v_classes\n",
    "\n",
    "print('Train size:', len(train_embeddings))\n",
    "print('Test size: ', len(test_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_predictions(probabilities, threshold=1):\n",
    "    # predictions based on probabilities!\n",
    "    my_predictions = []\n",
    "\n",
    "    probabilities_exp = np.exp(probabilities)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(predictions_test):\n",
    "        p_first = probabilities_exp[i]\n",
    "        p_second = probabilities_exp[i + 1]\n",
    "\n",
    "        p1 = p_first[0] + p_second[1]\n",
    "        p2 = p_first[1] + p_second[0]\n",
    "\n",
    "        if p1 > p2 * threshold:\n",
    "            my_predictions.append(0)\n",
    "        else:\n",
    "            my_predictions.append(1)\n",
    "        i += 2\n",
    "\n",
    "    return np.array(my_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "embedding_dim = train_embeddings.shape[-1]\n",
    "\n",
    "units = 1024\n",
    "num_labels = 2\n",
    "NUM_EPOCHS = 100\n",
    "NEGATIVE_SAMPLING = 7\n",
    "\n",
    "NUM_SAMPLES_TRAINING = len(train_embeddings)\n",
    "NUM_SAMPLES_VALIDATION = len(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading without negative sampling\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "\n",
    "def sample_negatives(embeddings, embeddings_batch, classes_batch, negative_sampling):\n",
    "    new_classes = []\n",
    "    new_embeddings_batch = []\n",
    "    for i, embedding in enumerate(embeddings_batch):\n",
    "\n",
    "        new_embeddings_batch.append(embedding)\n",
    "        new_classes.append(classes_batch[i])\n",
    "\n",
    "        for _ in range(negative_sampling):\n",
    "            new_embeddings_batch.append(\n",
    "                np.concatenate((embedding[:4], [random.choice(embeddings[:, 4, :])]), axis=0))\n",
    "            # negative class always\n",
    "            new_classes.append(1)\n",
    "    return np.array(new_embeddings_batch, dtype=np.float32), np.array(new_classes, dtype=np.int32)\n",
    "\n",
    "def generator(train, batch_size=64, negative_sampling=0):\n",
    "    \"\"\"\n",
    "    negative_sampling: For each positive sample these many negatives\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        embeddings = train_embeddings\n",
    "        classes = train_classes\n",
    "    else:\n",
    "        embeddings = test_embeddings\n",
    "        classes = test_classes\n",
    "\n",
    "    if train and negative_sampling > 0:\n",
    "        batch_size /= (negative_sampling + 1)\n",
    "        if batch_size != int(batch_size):\n",
    "            raise Exception('Batch size should be an integer. Please change negative sampling rate')\n",
    "\n",
    "        batch_size = int(batch_size)\n",
    "\n",
    "    # repeat\n",
    "    while(True):\n",
    "        if train:\n",
    "            embeddings, classes = shuffle(embeddings, classes)\n",
    "\n",
    "        length = len(embeddings)\n",
    "        for ndx in range(0, length, batch_size):\n",
    "            embeddings_batch = embeddings[ndx: min(ndx + batch_size, length)]\n",
    "            classes_batch = classes[ndx: min(ndx + batch_size, length)]\n",
    "\n",
    "            if negative_sampling <= 0:\n",
    "                yield embeddings_batch, classes_batch\n",
    "            else:\n",
    "                yield sample_negatives(embeddings, embeddings_batch, classes_batch, negative_sampling)\n",
    "\n",
    "def create_dataset(train, batch_size, negative_sampling):\n",
    "    dataset = tf.data.Dataset.from_generator(generator, (tf.float32, tf.int32), \n",
    "                                             output_shapes=(tf.TensorShape([None, 5, 4800]), tf.TensorShape([None])), \n",
    "                                             args = ([train, batch_size, negative_sampling]))\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "if train_on_validation:\n",
    "    print('Loading without negative sampling')\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((train_embeddings, train_classes))\n",
    "    dataset_train = dataset_train.shuffle(buffer_size=len(train_embeddings))\n",
    "\n",
    "    dataset_train = dataset_train.repeat()\n",
    "    dataset_train = dataset_train.batch(BATCH_SIZE)\n",
    "\n",
    "    iterator_train = dataset_train.make_one_shot_iterator()\n",
    "    train_X, train_y = iterator_train.get_next()\n",
    "\n",
    "    dataset_test = tf.data.Dataset.from_tensor_slices((test_embeddings, test_classes))\n",
    "\n",
    "    dataset_test = dataset_test.repeat()\n",
    "    dataset_test = dataset_test.batch(BATCH_SIZE)\n",
    "\n",
    "    iterator_test = dataset_test.make_one_shot_iterator()\n",
    "    test_X, test_y = iterator_test.get_next()\n",
    "else:\n",
    "    # use dummy NUM_EPOCHS + 1 to make predictions on train dataset also!\n",
    "    train_X, train_y = create_dataset(True, BATCH_SIZE, NEGATIVE_SAMPLING)\n",
    "\n",
    "    test_X, test_y = create_dataset(False, BATCH_SIZE, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(?, 5, 4800), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int64)\n",
      "\n",
      "Tensor(\"IteratorGetNext_1:0\", shape=(?, 5, 4800), dtype=float32)\n",
      "Tensor(\"IteratorGetNext_1:1\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(train_X)\n",
    "print(train_y)\n",
    "print()\n",
    "print(test_X)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention on top of gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        \n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        self.W1 = tf.keras.layers.Dense(units, name='Dense_1')\n",
    "        self.W2 = tf.keras.layers.Dense(units, name='Dense_2')\n",
    "        self.V = tf.keras.layers.Dense(1, name='Dense_3')\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayerWithSoftmax(tf.keras.Model):\n",
    "    def __init__(self, layers, num_classes, dropout_keep_proba=0.9, activation=tf.nn.relu):\n",
    "        super(DenseLayerWithSoftmax, self).__init__()\n",
    "        \n",
    "        self.dense_layers = []\n",
    "        self.dropout_keep_proba = dropout_keep_proba\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        for i, layer_size in enumerate(layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(layer_size, name='DenseLayer_' + str(i), use_bias=True, activation=tf.nn.relu))\n",
    "    \n",
    "        self.final_layer = tf.keras.layers.Dense(self.num_classes, name='DenseLayer_final', use_bias=True)\n",
    "    \n",
    "    def call(self, input_, input_labels):\n",
    "        \n",
    "        logits = input_\n",
    "        \n",
    "        for layer in self.dense_layers:\n",
    "            logits = layer(logits)\n",
    "            logits = tf.nn.dropout(logits, keep_prob=self.dropout_keep_proba)\n",
    "\n",
    "        logits = self.final_layer(logits)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(input_labels, depth=self.num_classes, dtype=tf.float32)\n",
    "        \n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "\n",
    "        per_example_loss = - tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        \n",
    "        return predicted_labels, loss, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNN(tf.keras.Model):\n",
    "    def __init__(self, units, attention_units=10, num_classes=2, fc_layers=[], dropout_keep_proba=0.9, activation=tf.nn.relu):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        \n",
    "        self.units = units\n",
    "        self.fc_layers = fc_layers\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.encoder = Encoder(self.units)\n",
    "        self.attention_layer = BahdanauAttention(attention_units)\n",
    "        self.feed_foward = DenseLayerWithSoftmax(fc_layers, num_classes, \n",
    "                                                 dropout_keep_proba=dropout_keep_proba, activation=activation)\n",
    "        \n",
    "    def call(self, input_embeddings, input_labels):\n",
    "\n",
    "        # sample input\n",
    "        sample_hidden = self.encoder.initialize_hidden_state(tf.shape(input_embeddings)[0])\n",
    "        sample_output, sample_hidden = self.encoder(input_embeddings, sample_hidden)\n",
    "        \n",
    "        sentence_to_check = tf.reshape(sample_output[:, -1, :], [-1, self.units])\n",
    "        first_four_sentences = sample_output[:, :4, :]\n",
    "        \n",
    "        attention_result, attention_weights = self.attention_layer(sentence_to_check, first_four_sentences)\n",
    "        \n",
    "        return self.feed_foward(attention_result, input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = []\n",
    "\n",
    "attentionRNN = AttentionRNN(units, attention_units=100, num_classes=2, fc_layers=fc_layers, dropout_keep_proba=0.9, activation=tf.nn.relu)\n",
    "\n",
    "predicted_labels_train, loss_train, log_probs_train = attentionRNN(train_X, train_y)\n",
    "predicted_labels_test, loss_test, log_probs_test = attentionRNN(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_parameters = 0\n",
    "# for variable in tf.trainable_variables():\n",
    "#     # shape is an array of tf.Dimension\n",
    "#     shape = variable.get_shape()\n",
    "#     print(variable.name)\n",
    "#     variable_parameters = 1\n",
    "#     for dim in shape:\n",
    "#         variable_parameters *= dim.value\n",
    "#     total_par\n",
    "# ameters += variable_parameters\n",
    "# print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "update_lr_every = int((math.ceil(NUM_SAMPLES_TRAINING / BATCH_SIZE) * NUM_EPOCHS) / 20)\n",
    "\n",
    "global_step  = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  1e-5,                # Base learning rate.\n",
    "  global_step,  # Current index into the dataset.\n",
    "  update_lr_every,          # Decay step.\n",
    "  0.96,                # Decay rate.\n",
    "  staircase=True)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, variables = zip(*optimizer.compute_gradients(loss_train))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n",
    "\n",
    "# train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss_train, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   1%|          | 45/4700 [00:06<09:36,  8.08it/s, epoch=1, iter_percent=1 %]   \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:00<00:10,  1.09it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:00<00:10,  1.09it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:00<00:10,  1.09it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:00<00:10,  1.09it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:00<00:10,  1.09it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:00<00:10,  1.09it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:00<00:10,  1.09it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :   8%|▊         | 1/12 [00:01<00:10,  1.09it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:01<00:02,  1.55it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:01<00:02,  1.55it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:01<00:02,  1.55it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:01<00:02,  1.55it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:01<00:02,  1.55it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   1%|          | 53/4700 [00:07<10:37,  7.28it/s, epoch=1, iter_percent=18 %]A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   2%|▏         | 93/4700 [00:08<02:51, 26.90it/s, epoch=2, iter_percent=0 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.81it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.81it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.81it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.81it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.81it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   2%|▏         | 101/4700 [00:09<03:08, 24.43it/s, epoch=2, iter_percent=17 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   3%|▎         | 137/4700 [00:10<02:06, 35.97it/s, epoch=3, iter_percent=1 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.56it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.56it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.56it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.56it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.56it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.56it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   3%|▎         | 145/4700 [00:10<03:11, 23.78it/s, epoch=3, iter_percent=18 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   4%|▍         | 185/4700 [00:11<02:05, 36.06it/s, epoch=4, iter_percent=0 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 75.58it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 75.58it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 75.58it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 75.58it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 75.58it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   4%|▍         | 193/4700 [00:12<03:04, 24.37it/s, epoch=4, iter_percent=17 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   5%|▍         | 233/4700 [00:13<02:32, 29.36it/s, epoch=5, iter_percent=1 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.95it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.95it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.95it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.95it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.95it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.95it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   5%|▌         | 241/4700 [00:13<02:58, 24.94it/s, epoch=5, iter_percent=18 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   6%|▌         | 277/4700 [00:14<02:02, 36.05it/s, epoch=6, iter_percent=0 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 72.94it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 72.94it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 72.94it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 72.94it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 72.94it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   6%|▌         | 285/4700 [00:15<03:01, 24.33it/s, epoch=6, iter_percent=17 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   7%|▋         | 325/4700 [00:16<02:00, 36.36it/s, epoch=7, iter_percent=1 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 69.80it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 69.80it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 69.80it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 69.80it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 69.80it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   7%|▋         | 333/4700 [00:16<03:03, 23.80it/s, epoch=7, iter_percent=18 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   8%|▊         | 369/4700 [00:17<02:02, 35.46it/s, epoch=8, iter_percent=0 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.80it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.80it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.80it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.80it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.80it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.80it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   8%|▊         | 377/4700 [00:18<03:06, 23.13it/s, epoch=8, iter_percent=17 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :   9%|▉         | 417/4700 [00:19<01:57, 36.33it/s, epoch=9, iter_percent=1 %] \n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.66it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.66it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.66it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.66it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.66it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  58%|█████▊    | 7/12 [00:00<00:00, 68.66it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :   9%|▉         | 425/4700 [00:19<02:58, 23.94it/s, epoch=9, iter_percent=18 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :  10%|▉         | 465/4700 [00:21<02:47, 25.26it/s, epoch=10, iter_percent=0 %]\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=0 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=8 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=16 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=25 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=33 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=41 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=50 %]\u001b[A\n",
      "predictions on validation :   0%|          | 0/12 [00:00<?, ?it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.33it/s, epoch=0, iter_percent=58 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.33it/s, epoch=0, iter_percent=66 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.33it/s, epoch=0, iter_percent=75 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.33it/s, epoch=0, iter_percent=83 %]\u001b[A\n",
      "predictions on validation :  67%|██████▋   | 8/12 [00:00<00:00, 73.33it/s, epoch=0, iter_percent=91 %]\u001b[A\n",
      "training phase            :  10%|█         | 472/4700 [00:21<03:11, 22.02it/s, epoch=10, iter_percent=17 %]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw predictions validation score 0.500 and unified 0.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training phase            :  10%|█         | 488/4700 [00:21<02:13, 31.60it/s, epoch=10, iter_percent=50 %]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-184cfa5fc48a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_cur\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_percent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%d %%\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_cur\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_SAMPLES_TRAINING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    def get_predictions(predicted_labels, y, log_probs, number_of_steps, num_samples, phase):\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        log_probs_total = []\n",
    "        \n",
    "        desc = 'predictions on ' + str(phase)\n",
    "        desc = desc.ljust(26)\n",
    "        with trange(number_of_steps, desc=desc) as t:\n",
    "            for i in t:\n",
    "                # display training status\n",
    "                epoch_cur = i * BATCH_SIZE // num_samples\n",
    "                iter_cur = (i * BATCH_SIZE ) % num_samples\n",
    "                t.set_postfix(epoch=epoch_cur,iter_percent=\"%d %%\"%(iter_cur/float(NUM_SAMPLES_VALIDATION)*100) )\n",
    "                \n",
    "                predictions_batch, true_labels_batch, log_probs_batch = sess.run([predicted_labels, y, log_probs])\n",
    "\n",
    "                predictions.append(predictions_batch)\n",
    "                true_labels.append(true_labels_batch)\n",
    "                log_probs_total.append(log_probs_batch)\n",
    "        \n",
    "        return np.concatenate(predictions, axis=0).reshape(-1), np.concatenate(true_labels, axis=0).reshape(-1), \\\n",
    "                    np.concatenate(log_probs_total, axis=0).reshape(-1, 2)\n",
    "    \n",
    "#     with trange(math.ceil(NUM_SAMPLES_TRAINING / (BATCH_SIZE / (NEGATIVE_SAMPLING + 1))) * NUM_EPOCHS) as t:\n",
    "    if train_on_validation:\n",
    "        number_of_steps = math.ceil(NUM_SAMPLES_TRAINING / BATCH_SIZE)\n",
    "        batch_index = BATCH_SIZE \n",
    "    else:\n",
    "        number_of_steps = math.ceil(NUM_SAMPLES_TRAINING / (BATCH_SIZE / (NEGATIVE_SAMPLING + 1)))\n",
    "        batch_index = BATCH_SIZE / (NEGATIVE_SAMPLING + 1)\n",
    "\n",
    "    desc = 'training phase'.ljust(26)\n",
    "    with trange(number_of_steps * NUM_EPOCHS, desc=desc) as t:\n",
    "        last_epoch = 0\n",
    "        for i in t:\n",
    "            # display training status\n",
    "            epoch_cur = (i * batch_index) // NUM_SAMPLES_TRAINING\n",
    "            iter_cur = (i * batch_index) % NUM_SAMPLES_TRAINING\n",
    "            t.set_postfix(epoch=epoch_cur,iter_percent=\"%d %%\"%(iter_cur/float(NUM_SAMPLES_TRAINING)*100) )\n",
    "            \n",
    "            _, _, lt = sess.run([train_op, global_step, loss_train])\n",
    "\n",
    "            losses.append(lt)\n",
    "\n",
    "            if epoch_cur > last_epoch:\n",
    "                last_epoch = epoch_cur\n",
    "                predictions_test, labels_test, probabilities_test = get_predictions(predicted_labels_test, test_y, log_probs_test, math.ceil(NUM_SAMPLES_VALIDATION / BATCH_SIZE), NUM_SAMPLES_VALIDATION, 'validation')\n",
    "                print(f'Raw predictions validation score {accuracy_score(labels_test, predictions_test):.3f} and unified {accuracy_score(labels_test[::2], get_final_predictions(probabilities_test, threshold=1)):.3f}')\n",
    "            \n",
    "    predictions_train, labels_train, probabilities_train = get_predictions(predicted_labels_train, train_y, log_probs_train, number_of_steps, NUM_SAMPLES_TRAINING, 'training')\n",
    "    predictions_test, labels_test, probabilities_test = get_predictions(predicted_labels_test, test_y, log_probs_test, math.ceil(NUM_SAMPLES_VALIDATION / BATCH_SIZE), NUM_SAMPLES_VALIDATION, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 15, 5\n",
    "\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(labels_train, predictions_train))\n",
    "print(accuracy_score(labels_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(labels_train, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(labels_test, predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(labels_test[::2], get_final_predictions(probabilities_test, threshold=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "[0.25019684 0.7498032 ]\n",
      "1 1\n",
      "[0.25019684 0.7498032 ]\n",
      "0 1\n",
      "[0.25016373 0.74983627]\n",
      "1 1\n",
      "[0.25016373 0.74983627]\n",
      "1 1\n",
      "[0.2501894 0.7498106]\n",
      "0 1\n",
      "[0.2501894 0.7498106]\n",
      "0 1\n",
      "[0.25013044 0.7498695 ]\n",
      "1 1\n",
      "[0.25013044 0.7498695 ]\n",
      "0 1\n",
      "[0.25010857 0.74989146]\n",
      "1 1\n",
      "[0.25010857 0.74989146]\n"
     ]
    }
   ],
   "source": [
    "0.6360235168359166 (neg_sam = 7)\n",
    "attentionRNN = AttentionRNN(units, attention_units=10, num_classes=2, fc_layers=[], dropout_keep_proba=0.9, activation=tf.nn.relu)\n",
    "\n",
    "predicted_labels_train, loss_train, log_probs_train = attentionRNN(train_X, train_y)\n",
    "predicted_labels_test, loss_test, log_probs_test = attentionRNN(test_X, test_y)\n",
    "    \n",
    "0.6424371993586317 (neg_sam = 7)\n",
    "\n",
    "attentionRNN = AttentionRNN(units, attention_units=10, num_classes=2, fc_layers=[64], dropout_keep_proba=0.9, activation=tf.nn.relu)\n",
    "\n",
    "predicted_labels_train, loss_train, log_probs_train = attentionRNN(train_X, train_y)\n",
    "predicted_labels_test, loss_test, log_probs_test = attentionRNN(test_X, test_y)\n",
    "\n",
    "0.6397648316408338 (neg_sam = 7)\n",
    "attentionRNN = AttentionRNN(units, attention_units=100, num_classes=2, fc_layers=[], dropout_keep_proba=0.9, activation=tf.nn.relu)\n",
    "\n",
    "predicted_labels_train, loss_train, log_probs_train = attentionRNN(train_X, train_y)\n",
    "predicted_labels_test, loss_test, log_probs_test = attentionRNN(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create skip thoughts embbedings (already done this once no need to do again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_batches(iterable, batch_size=64, do_shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate batches\n",
    "    Parameters\n",
    "    ----------\n",
    "    iterable: list\n",
    "        data to generate batches for\n",
    "    batch_size: int\n",
    "    do_shuffle: bool\n",
    "        Whether to shuffle in each epoch\n",
    "    \"\"\"\n",
    "    if do_shuffle:\n",
    "        iterable = shuffle(iterable)\n",
    "\n",
    "    length = len(iterable)\n",
    "    for ndx in range(0, length, batch_size):\n",
    "        iterable_batch = iterable[ndx: min(ndx + batch_size, length)]\n",
    "        yield iterable_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# CWD=/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo\n",
    "\n",
    "\n",
    "# # Directory to download the pretrained models to.\n",
    "# PRETRAINED_MODELS_DIR=\"${CWD}/skip_thoughts/pretrained/\"\n",
    "\n",
    "# mkdir -p ${PRETRAINED_MODELS_DIR}\n",
    "# cd ${PRETRAINED_MODELS_DIR}\n",
    "\n",
    "# # Download and extract the unidirectional model.\n",
    "# wget \"http://download.tensorflow.org/models/skip_thoughts_uni_2017_02_02.tar.gz\"\n",
    "# tar -xvf skip_thoughts_uni_2017_02_02.tar.gz\n",
    "# rm skip_thoughts_uni_2017_02_02.tar.gz\n",
    "\n",
    "# # Download and extract the bidirectional model.\n",
    "# wget \"http://download.tensorflow.org/models/skip_thoughts_bi_2017_02_16.tar.gz\"\n",
    "# tar -xvf skip_thoughts_bi_2017_02_16.tar.gz\n",
    "# rm skip_thoughts_bi_2017_02_16.tar.gz\n",
    "\n",
    "# %%bash\n",
    "\n",
    "# DIRECTORY=/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo/skip_thoughts_npy\n",
    "\n",
    "# wget http://www.cs.toronto.edu/~rkiros/models/dictionary.txt --directory-prefix=${DIRECTORY}\n",
    "# wget http://www.cs.toronto.edu/~rkiros/models/utable.npy --directory-prefix=${DIRECTORY}\n",
    "# wget http://www.cs.toronto.edu/~rkiros/models/btable.npy --directory-prefix=${DIRECTORY}\n",
    "# wget http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz --directory-prefix=${DIRECTORY}\n",
    "# wget http://www.cs.toronto.edu/~rkiros/models/uni_skip.npz.pkl --directory-prefix=${DIRECTORY}\n",
    "# wget http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz --directory-prefix=${DIRECTORY}\n",
    "# wget http://www.cs.toronto.edu/~rkiros/models/bi_skip.npz.pkl --directory-prefix=${DIRECTORY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skip_thoughts import load_model, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters...\n",
      "Compiling encoders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tables...\n",
      "Packing up...\n"
     ]
    }
   ],
   "source": [
    "# import skipthoughts\n",
    "\n",
    "model = load_model()\n",
    "encoder = Encoder(model)\n",
    "# model = skipthoughts.load_model()\n",
    "# encoder = skipthoughts.Encoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/sanagnos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11226"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_val) * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InputSentence1',\n",
       " 'InputSentence2',\n",
       " 'InputSentence3',\n",
       " 'InputSentence4',\n",
       " 'RandomFifthSentenceQuiz1',\n",
       " 'RandomFifthSentenceQuiz2']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['InputSentence' + str(i) for i in [1, 2, 3, 4]]\n",
    "names.append('RandomFifthSentenceQuiz1')\n",
    "names.append('RandomFifthSentenceQuiz2')\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11226,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_senteces = data_val[names].values.reshape(-1)\n",
    "all_senteces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new batch 0\n",
      "new batch 2048\n",
      "new batch 4096\n",
      "new batch 6144\n",
      "new batch 8192\n",
      "new batch 10240\n"
     ]
    }
   ],
   "source": [
    "all_senteces_encoded = []\n",
    "\n",
    "i = 0\n",
    "batch_size = 2048\n",
    "for sentences_batch in get_batches(all_senteces, batch_size=batch_size, do_shuffle=False):\n",
    "    print('new batch', i)\n",
    "    i += batch_size\n",
    "    all_senteces_encoded.append(encoder.encode(sentences_batch, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_senteces_encoded_np = np.concatenate(all_senteces_encoded, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.31560417e-04, -4.60474798e-03,  5.44821378e-03, ...,\n",
       "         -1.27321417e-02,  1.50634372e-03, -1.27441005e-03],\n",
       "        [-3.88688175e-04, -3.38627398e-02, -2.09803274e-03, ...,\n",
       "         -2.37040762e-02,  6.28774520e-03,  6.22180570e-03],\n",
       "        [ 7.84618035e-03,  2.99578756e-02, -3.70612345e-03, ...,\n",
       "         -1.77556872e-02,  2.97994260e-03,  1.15354499e-02],\n",
       "        [-2.13756808e-03, -9.57396440e-03, -2.61726649e-03, ...,\n",
       "         -1.27650155e-02,  2.76875636e-03, -3.84200877e-03],\n",
       "        [-7.14206742e-03, -2.27032006e-02,  1.57285631e-02, ...,\n",
       "         -3.76864523e-03,  2.76917825e-03,  6.08157646e-03],\n",
       "        [ 1.67411577e-03, -1.09987073e-02, -1.96930650e-03, ...,\n",
       "         -3.17759626e-02,  2.28350190e-03,  6.21051714e-03]],\n",
       "\n",
       "       [[ 3.60842608e-02,  4.32302430e-03,  1.93696190e-02, ...,\n",
       "         -7.76809687e-03,  2.66928691e-03,  7.26393657e-03],\n",
       "        [-8.56942497e-03, -5.56371734e-03,  1.18741076e-02, ...,\n",
       "          1.73653606e-02,  2.12568906e-03, -5.12142200e-03],\n",
       "        [-7.23170489e-03, -1.45057160e-02, -8.67060618e-04, ...,\n",
       "         -2.85060685e-02,  2.98341876e-03, -4.89283632e-03],\n",
       "        [-2.23819120e-03,  5.04488591e-03, -1.14408799e-03, ...,\n",
       "          2.55335961e-03,  2.66758469e-03, -4.35229391e-03],\n",
       "        [ 1.89843544e-04, -1.23545472e-02,  1.61505654e-03, ...,\n",
       "          2.44802106e-02,  2.79787229e-03, -1.02783293e-02],\n",
       "        [ 5.27526997e-03,  3.44697051e-02, -1.34914066e-03, ...,\n",
       "         -1.22210169e-02,  2.62495712e-03,  5.17406361e-03]],\n",
       "\n",
       "       [[ 8.06883257e-03, -2.81886663e-04, -1.77930724e-02, ...,\n",
       "         -8.57107621e-03,  2.07032845e-03, -8.87170061e-03],\n",
       "        [ 7.64323818e-03, -4.81079239e-03,  4.34369817e-02, ...,\n",
       "         -9.33142100e-03,  2.78433179e-03, -9.55881143e-04],\n",
       "        [-4.33776295e-03, -1.20636979e-02, -8.32437165e-03, ...,\n",
       "         -3.08302008e-02,  2.80257291e-03, -3.30258184e-03],\n",
       "        [-1.31324120e-03,  3.39875408e-02, -2.36461647e-02, ...,\n",
       "         -5.49418153e-03,  2.17832508e-03, -1.32339867e-03],\n",
       "        [-1.59501508e-02, -1.11152027e-02,  1.51782692e-03, ...,\n",
       "         -2.89176069e-02,  2.87730573e-03, -2.05327501e-03],\n",
       "        [-4.55980981e-03, -1.45514952e-02,  7.44268252e-03, ...,\n",
       "         -2.39367709e-02,  3.00620473e-03, -4.34638280e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.00421622e-02, -8.25255550e-03, -4.11984930e-03, ...,\n",
       "         -1.98362675e-02,  1.01864636e-02, -1.08193355e-02],\n",
       "        [ 5.77117503e-03, -2.05430444e-02,  1.05926730e-02, ...,\n",
       "         -1.95901711e-02,  3.13852797e-03, -2.24357750e-03],\n",
       "        [-9.01532883e-04, -4.20979271e-03,  9.15679522e-03, ...,\n",
       "         -4.18302715e-02,  2.91283336e-03,  9.65667143e-03],\n",
       "        [-3.80951865e-03, -5.28076664e-03,  1.66138634e-02, ...,\n",
       "         -7.27963168e-03,  5.43734943e-03, -1.16008474e-03],\n",
       "        [ 2.29017227e-03, -1.25600081e-02, -1.05442628e-02, ...,\n",
       "         -7.68475467e-03,  3.14846006e-03, -8.92997021e-04],\n",
       "        [ 8.97540431e-03, -8.56189802e-03,  1.70097537e-02, ...,\n",
       "         -3.76156420e-02,  4.74570040e-03, -1.98835623e-03]],\n",
       "\n",
       "       [[ 4.83984314e-03, -4.21830174e-03, -6.82792068e-03, ...,\n",
       "          7.77853727e-02,  2.33297609e-03, -6.75808219e-03],\n",
       "        [-2.25414895e-02, -1.05135087e-02, -4.16564895e-03, ...,\n",
       "         -4.44369018e-03,  3.11546889e-03,  1.62542928e-02],\n",
       "        [-2.48917174e-02, -1.34450141e-02, -5.48354490e-03, ...,\n",
       "          3.01340763e-02,  2.52627674e-03,  1.00381747e-02],\n",
       "        [ 1.81070738e-03, -9.37828328e-03, -2.15614936e-03, ...,\n",
       "         -1.00183710e-02,  1.92769815e-03,  4.11436707e-03],\n",
       "        [-2.74785813e-02, -6.33889157e-03, -1.46536902e-02, ...,\n",
       "          1.34628068e-03,  2.25980603e-03,  1.36969304e-02],\n",
       "        [ 3.08278343e-03, -1.24099804e-02, -7.09880982e-03, ...,\n",
       "         -8.90109339e-04,  1.31089520e-03, -8.54927767e-03]],\n",
       "\n",
       "       [[ 9.43316799e-03, -6.72865193e-03, -5.36974613e-03, ...,\n",
       "         -2.49442607e-02,  1.40496506e-03, -3.79904080e-03],\n",
       "        [ 3.80632244e-02, -1.27859078e-02,  1.39699923e-03, ...,\n",
       "         -8.68274830e-03,  3.43227433e-03, -2.96302256e-03],\n",
       "        [ 4.01667925e-03, -9.26739816e-03,  3.25085741e-04, ...,\n",
       "         -1.50916250e-02,  3.94415157e-03,  7.21574470e-05],\n",
       "        [ 4.77019610e-04, -3.73994745e-03, -2.73364573e-03, ...,\n",
       "          8.91980249e-03,  2.99269822e-03,  1.84082240e-03],\n",
       "        [ 5.26815699e-03, -1.64105706e-02,  1.98305268e-02, ...,\n",
       "         -5.94482059e-03,  3.08242720e-03, -2.68927915e-03],\n",
       "        [ 9.27577540e-03, -1.12122307e-02,  1.99576225e-02, ...,\n",
       "         -1.47719821e-02,  2.03355867e-03, -1.13680167e-02]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_senteces_encoded_np = all_senteces_encoded_np.reshape((-1, 6, 4800))\n",
    "all_senteces_encoded_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.save('/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo/skip-thoughts-embbedings_validation.npy', all_senteces_encoded_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
