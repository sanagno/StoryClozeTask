{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from model import NLUModel\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import sys\n",
    "import math\n",
    "\n",
    "DATA_DIRECTORY = '../data/'\n",
    "ROC_TRAIN_SET = 'train_stories.csv'\n",
    "ROC_VAL_SET = 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'\n",
    "ROC_TEST_SET = 'test_for_report-stories-labels.csv'\n",
    "\n",
    "TRAIN_SKIP_THOUGHTS_EMBEDDINGS = '/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo/skip-thoughts-embbedings.npy'\n",
    "VAL_SKIP_THOUGHTS_EMBEDDINGS = '/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo/skip-thoughts-embbedings_validation.npy'\n",
    "TEST_SKIP_THOUGHTS_EMBEDDINGS = '/cluster/project/infk/courses/machine_perception_19/Sasglentamekaiedo/skip-thoughts-embbedings_test.npy'\n",
    "\n",
    "LOG_PATH = '../log_path'\n",
    "\n",
    "# parameter\n",
    "num_labels = 2\n",
    "\n",
    "# create dataset\n",
    "def create_dataset_from_embeddings(embeddings, df):\n",
    "    v_embeddings = list()\n",
    "    v_classes = list()\n",
    "    correct_answers = df['AnswerRightEnding'].values\n",
    "\n",
    "    for i, story_embedding in enumerate(embeddings):\n",
    "        v_embeddings.append(np.append(story_embedding[:4], [story_embedding[4]], axis=0))\n",
    "        v_embeddings.append(np.append(story_embedding[:4], [story_embedding[5]], axis=0))\n",
    "\n",
    "        if correct_answers[i] == 1:\n",
    "            v_classes.append(0)\n",
    "            v_classes.append(1)\n",
    "        else:\n",
    "            v_classes.append(1)\n",
    "            v_classes.append(0)\n",
    "\n",
    "    return np.array(v_embeddings), np.array(v_classes)\n",
    "\n",
    "\n",
    "def get_final_predictions(probabilities):\n",
    "    # predictions based on probabilities!\n",
    "    my_predictions = []\n",
    "\n",
    "    probabilities_exp = np.exp(probabilities)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(probabilities):\n",
    "        p_first = probabilities_exp[i]\n",
    "        p_second = probabilities_exp[i + 1]\n",
    "\n",
    "        p1 = p_first[0] + p_second[1]\n",
    "        p2 = p_first[1] + p_second[0]\n",
    "\n",
    "        if p1 > p2:\n",
    "            my_predictions.append(0)\n",
    "        else:\n",
    "            my_predictions.append(1)\n",
    "        i += 2\n",
    "\n",
    "    return np.array(my_predictions)\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units))\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "        self.W1 = tf.keras.layers.Dense(units, name='Dense_1')\n",
    "        self.W2 = tf.keras.layers.Dense(units, name='Dense_2')\n",
    "        self.V = tf.keras.layers.Dense(1, name='Dense_3')\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class DenseLayerWithSoftmax(tf.keras.Model):\n",
    "    def __init__(self, layers, num_classes, dropout_keep_proba=0.9, activation=tf.nn.relu):\n",
    "        super(DenseLayerWithSoftmax, self).__init__()\n",
    "\n",
    "        self.dense_layers = []\n",
    "        self.dropout_keep_proba = dropout_keep_proba\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        for i, layer_size in enumerate(layers):\n",
    "            self.dense_layers.append(\n",
    "                tf.keras.layers.Dense(layer_size, name='DenseLayer_' + str(i), use_bias=True, activation=tf.nn.relu))\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(self.num_classes, name='DenseLayer_final', use_bias=True)\n",
    "\n",
    "    def call(self, input_, input_labels):\n",
    "\n",
    "        logits = input_\n",
    "\n",
    "        for layer in self.dense_layers:\n",
    "            logits = layer(logits)\n",
    "            logits = tf.nn.dropout(logits, keep_prob=self.dropout_keep_proba)\n",
    "\n",
    "        logits = self.final_layer(logits)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(input_labels, depth=self.num_classes, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "\n",
    "        per_example_loss = - tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        return predicted_labels, loss, log_probs\n",
    "\n",
    "\n",
    "class FCSkip(tf.keras.Model):\n",
    "    def __init__(self, units,num_classes=2, fc_layers=[], dropout_keep_prob=0.9, activation=tf.nn.relu):\n",
    "        super(FCSkip, self).__init__()\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "        self.encoder = Encoder(self.units)\n",
    "        self.feed_forward = DenseLayerWithSoftmax(fc_layers, num_classes,\n",
    "                                                 dropout_keep_proba=dropout_keep_prob, activation=activation)\n",
    "\n",
    "    def call(self, input_embeddings, input_labels):\n",
    "\n",
    "        # sample input\n",
    "        sample_hidden = self.encoder.initialize_hidden_state(tf.shape(input_embeddings)[0])\n",
    "        sample_output, sample_hidden = self.encoder(input_embeddings[:, :4, :], sample_hidden)\n",
    "\n",
    "        concatenated_input = tf.concat([sample_output[:, -1, :], input_embeddings[:, 4, :]], axis=1)\n",
    "\n",
    "        return self.feed_forward(concatenated_input, input_labels)\n",
    "\n",
    "\n",
    "class LSSkip(tf.keras.Model):\n",
    "    def __init__(self, units,num_classes=2, fc_layers=[], dropout_keep_prob=0.9, activation=tf.nn.relu):\n",
    "        super(LSSkip, self).__init__()\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "        self.feed_forward = DenseLayerWithSoftmax(fc_layers, num_classes,\n",
    "                                                 dropout_keep_proba=dropout_keep_prob, activation=activation)\n",
    "\n",
    "    def call(self, input_embeddings, input_labels):\n",
    "        concatenated_input = tf.concat([input_embeddings[:, 3, :], input_embeddings[:, 4, :]], axis=1)\n",
    "#         concatenated_input = input_embeddings[:, 3, :] + input_embeddings[:, 4, :]\n",
    "\n",
    "        return self.feed_forward(concatenated_input, input_labels)\n",
    "\n",
    "\n",
    "class SimpleAndEffectiveApproach(NLUModel):\n",
    "    train_embeddings=None\n",
    "    train_classes=None\n",
    "    test_embeddings=None\n",
    "    test_classes=None\n",
    "    final_test_embeddings=None\n",
    "    final_test_classes=None\n",
    "    batch_size=None\n",
    "\n",
    "    def __init__(self, units, fc_layers=None, num_classes=2, train_on_validation=False, mode='FC-skip', verbose=False, negative_sampling=3, learning_rate=1e-3):\n",
    "        assert mode in ['FC-skip', 'LS-skip'], \"mode specified not supported\"\n",
    "\n",
    "        super(SimpleAndEffectiveApproach, self).__init__('SimpleAndEffectiveApproach')\n",
    "        self.train_on_validation = train_on_validation\n",
    "        self.verbose = verbose\n",
    "        self.negative_sampling = negative_sampling\n",
    "        self.mode = mode\n",
    "        self.fc_layers = fc_layers\n",
    "        self.units = units\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def _create_graph(self):\n",
    "        train_x, train_y, test_x, test_y, final_test_x, final_test_y = self._prepare_embeddings()\n",
    "\n",
    "        if self.mode == 'FC-skip':\n",
    "            if self.fc_layers is None:\n",
    "                print('Using default values for feed forward network [256, 64]')\n",
    "                self.fc_layers = [256, 64]\n",
    "            fc_skip = FCSkip(self.units, num_classes=2, fc_layers=self.fc_layers,\n",
    "                                        dropout_keep_prob=0.9, activation=tf.nn.relu)\n",
    "\n",
    "            self.predicted_labels_train, self.loss_train, self.log_probs_train = fc_skip(train_x, train_y)\n",
    "            self.predicted_labels_test, self.loss_test, self.log_probs_test = fc_skip(test_x, test_y)\n",
    "            self.predicted_labels_final_test, self.loss_final_test, self.log_probs_final_test = fc_skip(final_test_x, final_test_y)\n",
    "\n",
    "        elif self.mode == 'LS-skip':\n",
    "            if self.fc_layers is None:\n",
    "                print('Using default values for feed forward network [2400, 1200, 600]')\n",
    "                self.fc_layers = [2400, 1200, 600]\n",
    "\n",
    "            ls_skip = LSSkip(self.units, num_classes=2, fc_layers=self.fc_layers, dropout_keep_prob=0.9, activation=tf.nn.relu)\n",
    "\n",
    "            self.predicted_labels_train, self.loss_train, self.log_probs_train = ls_skip(train_x, train_y)\n",
    "            self.predicted_labels_test, self.loss_test, self.log_probs_test = ls_skip(test_x, test_y)\n",
    "            self.predicted_labels_final_test, self.loss_final_test, self.log_probs_final_test = ls_skip(final_test_x, final_test_y)\n",
    "\n",
    "    def _prepare_embeddings(self):\n",
    "        if not self.train_on_validation:\n",
    "            data_train = pd.read_csv(os.path.join(DATA_DIRECTORY, ROC_TRAIN_SET), header='infer')\n",
    "            # has a shape (88161, 5, 4800)\n",
    "            train_embeddings = np.load(TRAIN_SKIP_THOUGHTS_EMBEDDINGS)\n",
    "        \n",
    "        data_test = pd.read_csv(os.path.join(DATA_DIRECTORY, ROC_TEST_SET), header='infer')\n",
    "        # has a shape (1871, 6, 4800)\n",
    "        test_embeddings = np.load(TEST_SKIP_THOUGHTS_EMBEDDINGS)\n",
    "\n",
    "        data_val = pd.read_csv(os.path.join(DATA_DIRECTORY, ROC_VAL_SET), header='infer')\n",
    "        # has a shape (1871, 6, 4800)\n",
    "        validation_embeddings = np.load(VAL_SKIP_THOUGHTS_EMBEDDINGS)\n",
    "\n",
    "        # create set for validation dataset\n",
    "        val_embeddings, val_classes = create_dataset_from_embeddings(validation_embeddings, data_val)\n",
    "        self.final_test_embeddings, self.final_test_classes = create_dataset_from_embeddings(test_embeddings, data_test)\n",
    "        \n",
    "        if self.train_on_validation:\n",
    "            self.train_embeddings, self.train_classes = shuffle(val_embeddings, val_classes)\n",
    "\n",
    "            self.test_embeddings, self.test_classes = self.final_test_embeddings, self.final_test_classes\n",
    "        else:\n",
    "            # noinspection PyUnboundLocalVariable\n",
    "            self.train_embeddings, self.train_classes = shuffle(train_embeddings), np.zeros(len(train_embeddings))\n",
    "\n",
    "            self.test_embeddings, self.test_classes = val_embeddings, val_classes\n",
    "\n",
    "        self.num_samples_training = len(self.train_embeddings)\n",
    "        self.num_samples_test = len(self.test_embeddings)\n",
    "        self.num_samples_final_test = len(self.final_test_embeddings)\n",
    "\n",
    "        if self.train_on_validation:\n",
    "            if self.verbose:\n",
    "                print('Loading without negative sampling.')\n",
    "                \n",
    "#             dataset_train = tf.data.Dataset.from_tensor_slices((self.train_embeddings, self.train_classes))\n",
    "#             dataset_train = dataset_train.shuffle(buffer_size=len(self.train_embeddings))\n",
    "\n",
    "#             dataset_train = dataset_train.batch(self.batch_size)\n",
    "#             dataset_train = dataset_train.repeat()\n",
    "\n",
    "#             iterator_train = dataset_train.make_one_shot_iterator()\n",
    "#             train_x, train_y = iterator_train.get_next()\n",
    "\n",
    "#             dataset_test = tf.data.Dataset.from_tensor_slices((self.test_embeddings, self.test_classes))\n",
    "\n",
    "#             dataset_test = dataset_test.batch(self.batch_size)\n",
    "#             dataset_test = dataset_test.repeat()\n",
    "\n",
    "#             iterator_test = dataset_test.make_one_shot_iterator()\n",
    "#             test_x, test_y = iterator_test.get_next()\n",
    "            train_x, train_y = self.create_dataset(0, self.batch_size, 0)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print('Loading with negative sampling on the original training set.')\n",
    "                \n",
    "            train_x, train_y = self.create_dataset(0, self.batch_size, self.negative_sampling)\n",
    "\n",
    "#             test_x, test_y = self.create_dataset(1, self.batch_size, 0)\n",
    "\n",
    "        # create dataset for the final test\n",
    "#         dataset_final_test = tf.data.Dataset.from_tensor_slices((self.final_test_embeddings, self.final_test_classes))\n",
    "\n",
    "#         dataset_final_test = dataset_final_test.batch(self.batch_size)\n",
    "#         dataset_final_test = dataset_final_test.repeat()\n",
    "\n",
    "#         iterator_final_test = dataset_final_test.make_one_shot_iterator()\n",
    "#         final_test_x, final_test_y = iterator_final_test.get_next()\n",
    "        test_x, test_y = self.create_dataset(1, self.batch_size, 0)\n",
    "        final_test_x, final_test_y = test_x, test_y = self.create_dataset(2, self.batch_size, 0)\n",
    "        \n",
    "        return train_x, train_y, test_x, test_y, final_test_x, final_test_y\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_negatives(embeddings, embeddings_batch, classes_batch, negative_sampling):\n",
    "        new_classes = []\n",
    "        new_embeddings_batch = []\n",
    "        for i, embedding in enumerate(embeddings_batch):\n",
    "\n",
    "            new_embeddings_batch.append(embedding)\n",
    "            new_classes.append(classes_batch[i])\n",
    "\n",
    "            for _ in range(negative_sampling):\n",
    "                new_embeddings_batch.append(\n",
    "                    np.concatenate((embedding[:4], [random.choice(embeddings[:, 4, :])]), axis=0))\n",
    "                # negative class always\n",
    "                new_classes.append(1)\n",
    "        return np.array(new_embeddings_batch, dtype=np.float32), np.array(new_classes, dtype=np.int32)\n",
    "\n",
    "    def generator(self, mode, batch_size=64, negative_sampling=0):\n",
    "        \"\"\"\n",
    "        negative_sampling: For each positive sample these many negatives\n",
    "        \"\"\"\n",
    "        if mode == 0:\n",
    "            embeddings = self.train_embeddings\n",
    "            classes = self.train_classes\n",
    "        elif mode == 1:\n",
    "            embeddings = self.test_embeddings\n",
    "            classes = self.test_classes\n",
    "        else:\n",
    "            embeddings = self.final_test_embeddings\n",
    "            classes = self.final_test_classes\n",
    "\n",
    "        if negative_sampling > 0:\n",
    "            batch_size /= (negative_sampling + 1)\n",
    "            if batch_size != int(batch_size):\n",
    "                raise Exception('Batch size should be an integer. Please change negative sampling rate')\n",
    "\n",
    "            batch_size = int(batch_size)\n",
    "\n",
    "        # repeat\n",
    "        while True:\n",
    "            if mode == 0 or (mode == 1 and self.train_on_validation):\n",
    "                embeddings, classes = shuffle(embeddings, classes)\n",
    "\n",
    "            length = len(embeddings)\n",
    "            for ndx in range(0, length, batch_size):\n",
    "                embeddings_batch = embeddings[ndx: min(ndx + batch_size, length)]\n",
    "                classes_batch = classes[ndx: min(ndx + batch_size, length)]\n",
    "\n",
    "                if negative_sampling <= 0:\n",
    "                    yield embeddings_batch, classes_batch\n",
    "                else:\n",
    "                    yield self.sample_negatives(embeddings, embeddings_batch, classes_batch, negative_sampling)\n",
    "\n",
    "    def create_dataset(self, mode, batch_size, negative_sampling):\n",
    "        dataset = tf.data.Dataset.from_generator(self.generator, (tf.float32, tf.int32),\n",
    "                                                 output_shapes=(\n",
    "                                                 tf.TensorShape([None, 5, 4800]), tf.TensorShape([None])),\n",
    "                                                 args=([mode, batch_size, negative_sampling]))\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "\n",
    "    def _predict_wth_session(self, sess, final_test=False):\n",
    "        predictions = []\n",
    "        for _ in range(math.ceil(self.num_samples_test / self.batch_size)):\n",
    "            \n",
    "            if final_test:\n",
    "                predictions_batch = sess.run(self.log_probs_final_test)\n",
    "            else:\n",
    "                predictions_batch = sess.run(self.log_probs_test)\n",
    "                \n",
    "            predictions.append(predictions_batch)\n",
    "\n",
    "        return np.concatenate(predictions, axis=0).reshape(-1, 2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X should always be the validation set?\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, tf.train.latest_checkpoint(LOG_PATH))\n",
    "\n",
    "            return self._predict_wth_session(sess, final_test=True)\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=64):\n",
    "        \"\"\"\n",
    "        X, y are unused in this case\n",
    "        \"\"\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self._create_graph()\n",
    "        \n",
    "        update_learning_rate = 20 # empirical\n",
    "        if self.train_on_validation:\n",
    "            update_lr_every = int((math.ceil(self.num_samples_training / batch_size) * epochs) / update_learning_rate)\n",
    "        else:\n",
    "            update_lr_every = int(\n",
    "                (math.ceil(self.num_samples_training / (batch_size / (self.negative_sampling + 1))) * epochs) / update_learning_rate)\n",
    "\n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "        if self.verbose:\n",
    "            print('Updating the learning rate every:', update_lr_every, 'steps.')\n",
    "\n",
    "        # learning rate 1e-3 for most models\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            self.learning_rate,  # Base learning rate.\n",
    "            global_step,  # Current index into the dataset.\n",
    "            update_lr_every,  # Decay step.\n",
    "            0.96,  # Decay rate.\n",
    "            staircase=True)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        gradients, variables = zip(*optimizer.compute_gradients(self.loss_train))\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, variables), global_step=global_step)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # define model saver\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            #     with trange(math.ceil(NUM_SAMPLES_TRAINING / (batch_size / (negative_sampling + 1))) * NUM_EPOCHS) as t:\n",
    "            if self.train_on_validation:\n",
    "                number_of_steps = math.ceil(self.num_samples_training / batch_size)\n",
    "            else:\n",
    "                number_of_steps = math.ceil(self.num_samples_training/ (batch_size / (self.negative_sampling + 1)))\n",
    "\n",
    "            last_epoch = 0\n",
    "            for i in range(number_of_steps * epochs):\n",
    "                # display training status\n",
    "                epoch_cur = i // number_of_steps\n",
    "\n",
    "                _, _, lt = sess.run([train_op, global_step, self.loss_train])\n",
    "\n",
    "                if epoch_cur > last_epoch and self.verbose:\n",
    "                    # print score on validation set\n",
    "                    last_epoch = epoch_cur\n",
    "                    log_predictions_test = self._predict_wth_session(sess)\n",
    "                    \n",
    "                    score = accuracy_score(self.test_classes[::2], get_final_predictions(log_predictions_test))\n",
    "                    print('At epoch %3d score on test set %.3f' % (last_epoch, score))\n",
    "\n",
    "            saver.save(sess,os.path.join(LOG_PATH,\"model\"),\n",
    "                       global_step=int(epochs * self.num_samples_training / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleAndEffectiveApproach = SimpleAndEffectiveApproach(4800, train_on_validation=False, mode='LS-skip', verbose=True, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading with negative sampling on the original training set.\n",
      "Using default values for feed forward network [2400, 1200, 600]\n",
      "Updating the learning rate every: 2755 steps.\n",
      "At epoch   1 score on test set 0.500\n",
      "At epoch   2 score on test set 0.497\n",
      "At epoch   3 score on test set 0.486\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-72532c050bb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimpleAndEffectiveApproach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-31c94b8a436f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, batch_size)\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0mepoch_cur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnumber_of_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepoch_cur\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlast_epoch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simpleAndEffectiveApproach.fit(None, None, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predictions = simpleAndEffectiveApproach.predict(None)\n",
    "\n",
    "score = accuracy_score(simpleAndEffectiveApproach.final_test_classes[::2], get_final_predictions(log_predictions))\n",
    "\n",
    "print('Final score on test set based on last epoch model %.5f' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e-3 for train on validation\n",
    "1e-7 for train on training ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
