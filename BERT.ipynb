{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation: we perform synonym and random\n",
    "word replacement with NLTK and WordNet on the contexts\n",
    "of the SQuAD dataset. Questions are left unchanged. We\n",
    "explored different strategies of synonym replacement\n",
    "(sampling rate, +random words, +stop words) and\n",
    "injected different amount of augmented data (x0.33, x1, x2,\n",
    "x3) on top of the original data in our experiments.\n",
    "● For each word in a context paragraph\n",
    "○ 20% of the time:\n",
    "call replace_synonym\n",
    "■ if exists synonyms:\n",
    "replace with a random synonym\n",
    "■ otherwise:\n",
    "replace with a random word\n",
    "○ 80% of the time: remain unchanged\n",
    "\n",
    "http://web.stanford.edu/class/cs224n/posters/15845024.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # suppress some deprecation warnings\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_directory = './data'\n",
    "\n",
    "data_val = pd.read_csv(os.path.join(data_directory, 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'), header='infer')\n",
    "data_test = pd.read_csv(os.path.join(data_directory, 'cloze_test_test__spring2016 - cloze_test_ALL_test.csv'), header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0525 16:20:21.492210 47019843706304 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/sanagnos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cluster/home/sanagnos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /cluster/home/sanagnos/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_val):\n",
    "    contexts = list()\n",
    "    last_sentences = list()\n",
    "    classes = list()\n",
    "    for pos in range(len(data_val)):\n",
    "        story_start = data_val.iloc[pos][['InputSentence' + str(i) for i in [1, 2, 3, 4]]].values\n",
    "        \n",
    "        contexts.append(\" \".join(story_start))\n",
    "        last_sentences.append(data_val.iloc[pos]['RandomFifthSentenceQuiz1'])\n",
    "        contexts.append(\" \".join(story_start))\n",
    "        last_sentences.append(data_val.iloc[pos]['RandomFifthSentenceQuiz2'])\n",
    "        \n",
    "        if data_val.iloc[pos]['AnswerRightEnding'] == 1:\n",
    "            classes.append(0)\n",
    "            classes.append(1)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "            classes.append(0)\n",
    "            \n",
    "    return pd.DataFrame({'story': contexts, 'ending': last_sentences, 'class': classes})\n",
    "\n",
    "val_pd = create_dataset(data_val)\n",
    "test_pd = create_dataset(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3742\n",
      "3742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train = shuffle(val_pd)\n",
    "train_unshuffled = val_pd\n",
    "test = test_pd\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_COLUMN = 'story'\n",
    "ENDING_COLUMN = 'ending'\n",
    "LABEL_COLUMN = 'class'\n",
    "\n",
    "# label_list is 0 for a true story and 1 for a false story\n",
    "label_list = [0, 1]\n",
    "\n",
    "REPLICATION_FACTOR = 5\n",
    "\n",
    "train_InputExamples = pd.concat([train]*REPLICATION_FACTOR).apply(lambda x: \n",
    "                                                                  bert.run_classifier.InputExample(guid=None,\n",
    "                                                                  text_a = x[CONTEXT_COLUMN], \n",
    "                                                                  text_b = x[ENDING_COLUMN], \n",
    "                                                                  label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[CONTEXT_COLUMN], \n",
    "                                                                   text_b = x[ENDING_COLUMN], \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert.run_classifier import PaddingInputExample, _truncate_seq_pair, InputFeatures\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "def replace_with_synonym(token, tokenizer):\n",
    "  new_token = token\n",
    "  synonyms = []\n",
    "  for syn in wordnet.synsets(token):\n",
    "    for l in syn.lemmas():\n",
    "      synonyms.append(l.name())\n",
    "  if len(synonyms) > 0:\n",
    "    new_token = tokenizer.tokenize(random.choice(synonyms))[0]\n",
    "#     print(token, new_token)\n",
    "  return new_token\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer, set_synonyms=False, percentage_synonyms=0.2):\n",
    "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "  \n",
    "  if set_synonyms == False:\n",
    "    percentage_synonyms = 0\n",
    "\n",
    "  features = []\n",
    "  for (ex_index, example) in enumerate(examples):\n",
    "    if ex_index % 10000 == 0:\n",
    "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "    feature = convert_single_example(ex_index, example, label_list,\n",
    "                                     max_seq_length, tokenizer, percentage_synonyms)\n",
    "\n",
    "    features.append(feature)\n",
    "  return features\n",
    "\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                           tokenizer, percentage_synonyms):\n",
    "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "  if isinstance(example, PaddingInputExample):\n",
    "    return InputFeatures(\n",
    "        input_ids=[0] * max_seq_length,\n",
    "        input_mask=[0] * max_seq_length,\n",
    "        segment_ids=[0] * max_seq_length,\n",
    "        label_id=0,\n",
    "        is_real_example=False)\n",
    "\n",
    "  # Which tokens to replace with synonyms\n",
    "  set_synonyms = np.random.choice([True, False], max_seq_length,\n",
    "                                  p=[percentage_synonyms, 1 - percentage_synonyms])\n",
    "\n",
    "  label_map = {}\n",
    "  for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "  tokens_a = tokenizer.tokenize(example.text_a)\n",
    "  tokens_b = None\n",
    "  if example.text_b:\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "  if tokens_b:\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "  else:\n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "  # The convention in BERT is:\n",
    "  # (a) For sequence pairs:\n",
    "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "  # (b) For single sequences:\n",
    "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "  #  type_ids: 0     0   0   0  0     0 0\n",
    "  #\n",
    "  # Where \"type_ids\" are used to indicate whether this is the first\n",
    "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "  # embedding vector (and position vector). This is not *strictly* necessary\n",
    "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "  # it easier for the model to learn the concept of sequences.\n",
    "  #\n",
    "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "  # used as the \"sentence vector\". Note that this only makes sense because\n",
    "  # the entire model is fine-tuned.\n",
    "  tokens = []\n",
    "  segment_ids = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  index = 1\n",
    "  for token in tokens_a:\n",
    "    if set_synonyms[index]:\n",
    "        tokens.append(replace_with_synonym(token, tokenizer))\n",
    "    else:\n",
    "        tokens.append(token)\n",
    "    segment_ids.append(0)\n",
    "    index += 1\n",
    "  tokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "  index += 1\n",
    "\n",
    "  if tokens_b:\n",
    "    for token in tokens_b:\n",
    "      if set_synonyms[index]:\n",
    "        tokens.append(replace_with_synonym(token, tokenizer))\n",
    "      else:\n",
    "        tokens.append(token)\n",
    "      segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "  # tokens are attended to.\n",
    "  input_mask = [1] * len(input_ids)\n",
    "\n",
    "  # Zero-pad up to the sequence length.\n",
    "  while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "\n",
    "  label_id = label_map[example.label]\n",
    "  if ex_index < 5:\n",
    "    tf.logging.info(\"*** Example ***\")\n",
    "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "        [tokenization.printable_text(x) for x in tokens]))\n",
    "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "  feature = InputFeatures(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids,\n",
    "      label_id=label_id,\n",
    "      is_real_example=True)\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most this tokens long.\n",
    "MAX_SEQ_LENGTH = 96\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer,\n",
    "                                              set_synonyms=True, percentage_synonyms=0.2)\n",
    "\n",
    "test_features = convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023, 2182, 1005, 1055, 2019, 2742, 1997, 2478, 1996, 14324, 19204, 17629, 1064, 1064, 1064, 3231]\n",
      "[2023, 2182, 1005, 1055, 2019, 2742, 1997, 2478, 1996, 14324, 19204, 17629, 1064, 1064, 1064, 3231, 101]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"This here's an example of using the BERT tokenizer ||| test\")\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))\n",
    "tokens.append(\"[CLS]\")\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(tf.keras.Model):\n",
    "    def __init__(self, layers, dropout_keep_proba=0.9, activation=tf.nn.relu):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        \n",
    "        self.dense_layers = []\n",
    "        self.dropout_keep_proba = dropout_keep_proba\n",
    "        \n",
    "        for i, layer_size in enumerate(layers):\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(layer_size, name='DenseLayer_' + str(i), use_bias=True, activation=tf.nn.relu))\n",
    "    \n",
    "    def call(self, logits):\n",
    "        \n",
    "        for layer in self.dense_layers:\n",
    "            logits = layer(logits)\n",
    "            logits = tf.nn.dropout(logits, keep_prob=self.dropout_keep_proba)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "  n_ctx = input_ids.shape[-1].value\n",
    " \n",
    "  transformer_outputs = bert_outputs['sequence_output']\n",
    "\n",
    "  # final output size\n",
    "  weight_size_last_sentence = hidden_size * 3\n",
    "  layers_for_last_sentence = [weight_size_last_sentence]\n",
    "  assert weight_size_last_sentence == layers_for_last_sentence[-1]\n",
    "  dense_last_sentence = DenseLayer(layers_for_last_sentence)\n",
    "    \n",
    "  segment_ids_expandend = tf.tile(tf.expand_dims(segment_ids, 2), [1, 1, weight_size_last_sentence])\n",
    "  segment_ids_expandend = tf.cast(segment_ids_expandend, tf.float32)\n",
    "  result = dense_last_sentence(transformer_outputs) * segment_ids_expandend\n",
    "\n",
    "  output_layer_last_sentence = tf.reduce_sum(result, 1)\n",
    "        \n",
    "  output_layer = output_layer_last_sentence\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "    logits = tf.layers.dense(output_layer, 512, use_bias=True, activation=tf.nn.sigmoid)\n",
    "    logits = tf.nn.dropout(logits, keep_prob=0.9)\n",
    "    logits = tf.layers.dense(logits, num_labels, use_bias=True)\n",
    "\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    \n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_steps 701\n"
     ]
    }
   ],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 50000\n",
    "SAVE_SUMMARY_STEPS = 100000\n",
    "\n",
    "OUTPUT_DIR = 'output_dir'\n",
    "SAVE_RESULTS_DIR = 'results_predictions'\n",
    "\n",
    "N_ESTIMATORS = 15\n",
    "\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / REPLICATION_FACTOR / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "assert REPLICATION_FACTOR == int(NUM_TRAIN_EPOCHS)\n",
    "\n",
    "print('num_train_steps', num_train_steps)\n",
    "\n",
    "# Skip this step to avoid disk quota\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n",
    "\n",
    "\n",
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_predictions(in_contexts, in_last_sentences):\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = y, label = 0) for x, y in zip(in_contexts, in_last_sentences)] # here, \"\" is just a dummy label\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    predictions = [prediction['probabilities'] for prediction in predictions]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def combine_predictions(predictions):\n",
    "    my_predictions = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(test):\n",
    "        p_first = np.exp(predictions[i])\n",
    "        p_second = np.exp(predictions[i + 1])\n",
    "\n",
    "        p1 = p_first[0] + p_second[1]\n",
    "        p2 = p_first[1] + p_second[0]\n",
    "\n",
    "        if p1 > p2:\n",
    "            my_predictions.append(1)\n",
    "        else:\n",
    "            my_predictions.append(2)\n",
    "        i += 2\n",
    "        \n",
    "    return np.array(my_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= BEGINNING TRAINING FOR CLASSIFIER  0 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/sanagnos/.local/lib64/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:04:39.547921\n",
      "Score on train is  0.9935863174772849\n",
      "Score on val is  0.8818813468733298\n",
      "========= BEGINNING TRAINING FOR CLASSIFIER  1 =========\n",
      "Training took time  0:04:41.831737\n",
      "Score on train is  0.997327632282202\n",
      "Score on val is  0.8760021378941742\n",
      "========= BEGINNING TRAINING FOR CLASSIFIER  2 =========\n",
      "Training took time  0:04:39.743914\n",
      "Score on train is  0.9941207910208445\n",
      "Score on val is  0.882950293960449\n",
      "========= BEGINNING TRAINING FOR CLASSIFIER  3 =========\n"
     ]
    }
   ],
   "source": [
    "!rm -rf {SAVE_RESULTS_DIR} || true\n",
    "!mkdir {SAVE_RESULTS_DIR}\n",
    "\n",
    "true_labels_train = train_unshuffled['class'].values[::2] + 1\n",
    "true_labels_val = test['class'].values[::2] + 1\n",
    "\n",
    "for i in range(N_ESTIMATORS):\n",
    "    !rm -rf {OUTPUT_DIR} || true\n",
    "    \n",
    "    train_features = shuffle(train_features)\n",
    "    \n",
    "    # Create an input function for training. drop_remainder = True for using TPUs.\n",
    "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    \n",
    "    print(f'========= BEGINNING TRAINING FOR CLASSIFIER {i:2d} =========')\n",
    "    current_time = datetime.now()\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "    print(\"Training took time \", datetime.now() - current_time)\n",
    "        \n",
    "    predictions = get_final_predictions(train_unshuffled['story'].values, train_unshuffled['ending'].values)\n",
    "    print('Score on train is ', accuracy_score(true_labels_train, combine_predictions(predictions)))\n",
    "    np.savetxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, \"predictions_train_\" + str(i) + '.csv'), predictions, delimiter=\",\")\n",
    "    \n",
    "    predictions = get_final_predictions(test['story'].values, test['ending'].values)\n",
    "    val_score = accuracy_score(true_labels_val, combine_predictions(predictions))\n",
    "    print('Score on val is ', val_score)\n",
    "    np.savetxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, \"predictions_test_\" + str(val_score) + '_classifier_' + str(i) + '.csv'), predictions, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL 12 EPOCHS unlesss specified otherwise\n",
    "\n",
    "## just take pooled output from BERT:\n",
    "0.8562266167824693\n",
    "0.8722608230892571\n",
    "0.8583645109567076\n",
    "0.8599679315873864\n",
    "\n",
    "\n",
    "## only last sentece layers = [hidden_size]:\n",
    "0.8679850347407804\n",
    "0.8711918760021379\n",
    "0.877605558524853\n",
    "0.8765366114377339\n",
    "0.8674505611972207\n",
    "\n",
    "## same but for 3 epochs\n",
    "0.8760021378941742\n",
    "0.8722608230892571\n",
    "0.8743987172634955\n",
    "0.8797434526990914\n",
    "0.8797434526990914\n",
    "\n",
    "## only last sentece layers = [hidden_size * 5] 3 epochs:\n",
    "0.8813468733297701\n",
    "0.8818813468733298\n",
    "0.8733297701763763\n",
    "0.8808123997862106\n",
    "0.8733297701763763\n",
    "0.8738642437199359\n",
    "\n",
    "## only last sentece 3 epochs:[hidden_size * 5, hidden_size * 3, weight_size_last_sentence]\n",
    "0.8786745056119722\n",
    "0.8717263495456975\n",
    "\n",
    "## only last sentece layers = [hidden_size] 3 epochs with random_replacement 0.1:\n",
    "0.8850881881346874\n",
    "0.8743987172634955\n",
    "0.8685195082843399\n",
    "0.882950293960449\n",
    "\n",
    "## only last sentece layers = [hidden_size * 3] 3 epochs with random_replacement 0.2:\n",
    "0.8818813468733298\n",
    "0.8760021378941742\n",
    "0.882950293960449\n",
    "\n",
    "## concat of last sentence and context [hidden_size], [hidden_size] 3 epochs:\n",
    "0.8711918760021379\n",
    "0.8711918760021379\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 6. 9.]\n",
      " [2. 4. 6.]]\n",
      "[[2. 4. 6.]\n",
      " [3. 6. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# TEST \n",
    "\n",
    "mask = tf.constant([[0,0,1,1,1,0],\n",
    "                    [0,0,0,1,1,0]])\n",
    "\n",
    "mask1 = tf.constant([[1,1,1,1,1,0],\n",
    "                     [1,1,1,1,1,0]])\n",
    "\n",
    "\n",
    "features = tf.ones([2,6,3], dtype=tf.float32)\n",
    "\n",
    "# weights = tf.get_variable('weights', [3,3])\n",
    "weights = tf.constant([1,2,3], dtype=tf.float32)\n",
    "\n",
    "mask_expanded = tf.tile(tf.expand_dims(mask, 2), [1,1,3])\n",
    "mask_expanded = tf.cast(mask_expanded, tf.float32)\n",
    "r = (features * weights) * mask_expanded\n",
    "\n",
    "r = tf.reduce_sum(r, 1)\n",
    "\n",
    "mask_expanded1 = tf.tile(tf.expand_dims((mask1 * (1 - mask)), 2), [1,1,3])\n",
    "mask_expanded1 = tf.cast(mask_expanded1, tf.float32)\n",
    "r1 = (features * weights) * mask_expanded1\n",
    "\n",
    "r1 = tf.reduce_sum(r1, 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(r))\n",
    "    print(sess.run(r1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For classifier  0 train accuracy 1.0000 and test accuracy 0.8787\n",
      "For classifier  1 train accuracy 1.0000 and test accuracy 0.8899\n",
      "For classifier  2 train accuracy 1.0000 and test accuracy 0.8867\n",
      "For classifier  3 train accuracy 1.0000 and test accuracy 0.8771\n",
      "For classifier  4 train accuracy 0.9995 and test accuracy 0.8904\n",
      "For classifier  5 train accuracy 1.0000 and test accuracy 0.8883\n",
      "For classifier  6 train accuracy 1.0000 and test accuracy 0.8856\n",
      "For classifier  7 train accuracy 1.0000 and test accuracy 0.8840\n",
      "For classifier  8 train accuracy 1.0000 and test accuracy 0.8830\n",
      "For classifier  9 train accuracy 1.0000 and test accuracy 0.8942\n",
      "For classifier 10 train accuracy 1.0000 and test accuracy 0.8985\n",
      "For classifier 11 train accuracy 1.0000 and test accuracy 0.8910\n",
      "For classifier 12 train accuracy 1.0000 and test accuracy 0.8936\n",
      "For classifier 13 train accuracy 1.0000 and test accuracy 0.8936\n",
      "For classifier 14 train accuracy 1.0000 and test accuracy 0.8862\n",
      "ENSEMBLE ACCURACY MODE\n",
      "1.0\n",
      "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
      "1.0\n",
      "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
      "1.0\n",
      "\n",
      "ENSEMBLE ACCURACY MODE\n",
      "0.9102084446819882\n",
      "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
      "0.9064671298770711\n",
      "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
      "0.9075360769641903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "SAVE_RESULTS_DIR = 'results_predictions'\n",
    "\n",
    "\n",
    "files = [f for f in listdir(SAVE_RESULTS_DIR) if isfile(join(SAVE_RESULTS_DIR, f))]\n",
    "\n",
    "true_labels_train = train_unshuffled['class'].values[::2] + 1\n",
    "true_labels_test = test['class'].values[::2] + 1\n",
    "\n",
    "classifiers = [int(file.split(\"_\")[2].split(\".\")[0]) for file in files]\n",
    "num_classifiers = np.max(classifiers)\n",
    "\n",
    "predictions_train = []\n",
    "predictions_test = []\n",
    "for i in range(num_classifiers + 1):\n",
    "    predictions_file_train = np.genfromtxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, \n",
    "                                                        \"predictions_train_\" + str(i) + '.csv'), delimiter=',')\n",
    "    predictions_train.append(predictions_file_train)\n",
    "    \n",
    "    test_file = [x for x in files if 'classifier_' + str(i) in x][0]\n",
    "    predictions_file_test = np.genfromtxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, test_file), delimiter=',')\n",
    "    predictions_test.append(predictions_file_test)\n",
    "    print(f'For classifier {i:2d} train accuracy {accuracy_score(true_labels_train, combine_predictions(predictions_file_train)):.4f} and test accuracy {accuracy_score(true_labels_test, combine_predictions(predictions_file_test)):.4f}')\n",
    "    \n",
    "def print_ensemble_predictions(predictions, true_labels):\n",
    "    preds_mode = [combine_predictions(p) for p in predictions]\n",
    "    preds_mode = np.array(preds_mode)\n",
    "    preds_mode = stats.mode(preds_mode)[0][0]\n",
    "\n",
    "    print('ENSEMBLE ACCURACY MODE')\n",
    "    print(accuracy_score(true_labels, preds_mode))\n",
    "\n",
    "    preds_prob = np.mean(predictions, axis=0)\n",
    "    preds_prob = combine_predictions(preds_prob)\n",
    "\n",
    "    print('ENSEMBLE ACCURACY PROB MEAN ON LOGS')\n",
    "    print(accuracy_score(true_labels, preds_prob))\n",
    "\n",
    "\n",
    "    preds_prob = np.log(np.mean(np.exp(predictions), axis=0))\n",
    "    preds_prob = combine_predictions(preds_prob)\n",
    "\n",
    "    print('ENSEMBLE ACCURACY PROB MEAN ON PROBS')\n",
    "    print(accuracy_score(true_labels, preds_prob))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "print_ensemble_predictions(predictions_train, true_labels_train)\n",
    "print_ensemble_predictions(predictions_test, true_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85567284, -0.55338029],\n",
       "       [-0.87034223, -0.54267442],\n",
       "       [-0.60520196, -0.78957908],\n",
       "       ...,\n",
       "       [-1.29842089, -0.31877721],\n",
       "       [-1.00223294, -0.4573779 ],\n",
       "       [-0.17982265, -1.80434856]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### OLD\n",
    "\n",
    "\n",
    "small_bert (whole val and whole test)\n",
    "\n",
    "output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "logits = tf.layers.dense(output_layer, 128, use_bias=True, activation=tf.nn.sigmoid)\n",
    "logits = tf.nn.dropout(logits, keep_prob=0.9)\n",
    "logits = tf.layers.dense(logits, num_labels, use_bias=True)\n",
    "\n",
    "For classifier  0 train accuracy 1.0000 and test accuracy 0.8391\n",
    "For classifier  1 train accuracy 1.0000 and test accuracy 0.8343\n",
    "For classifier  2 train accuracy 1.0000 and test accuracy 0.8386\n",
    "For classifier  3 train accuracy 1.0000 and test accuracy 0.8311\n",
    "For classifier  4 train accuracy 1.0000 and test accuracy 0.8434\n",
    "For classifier  5 train accuracy 1.0000 and test accuracy 0.8397\n",
    "For classifier  6 train accuracy 1.0000 and test accuracy 0.8434\n",
    "For classifier  7 train accuracy 1.0000 and test accuracy 0.8418\n",
    "For classifier  8 train accuracy 1.0000 and test accuracy 0.8359\n",
    "For classifier  9 train accuracy 1.0000 and test accuracy 0.8487\n",
    "For classifier 10 train accuracy 1.0000 and test accuracy 0.8332\n",
    "For classifier 11 train accuracy 1.0000 and test accuracy 0.8429\n",
    "For classifier 12 train accuracy 1.0000 and test accuracy 0.8397\n",
    "For classifier 13 train accuracy 1.0000 and test accuracy 0.8407\n",
    "For classifier 14 train accuracy 1.0000 and test accuracy 0.8365\n",
    "ENSEMBLE ACCURACY MODE\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "1.0\n",
    "\n",
    "ENSEMBLE ACCURACY MODE\n",
    "0.8722608230892571\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "0.8786745056119722\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "0.8786745056119722\n",
    "\n",
    "big_bert (whole val and whole test)\n",
    "For classifier  0 train accuracy 1.0000 and test accuracy 0.8691\n",
    "For classifier  1 train accuracy 1.0000 and test accuracy 0.8664\n",
    "For classifier  2 train accuracy 1.0000 and test accuracy 0.8680\n",
    "For classifier  3 train accuracy 1.0000 and test accuracy 0.8562\n",
    "For classifier  4 train accuracy 0.9984 and test accuracy 0.8397\n",
    "For classifier  5 train accuracy 1.0000 and test accuracy 0.8696\n",
    "For classifier  6 train accuracy 1.0000 and test accuracy 0.8381\n",
    "For classifier  7 train accuracy 1.0000 and test accuracy 0.8589\n",
    "For classifier  8 train accuracy 1.0000 and test accuracy 0.8632\n",
    "For classifier  9 train accuracy 1.0000 and test accuracy 0.8600\n",
    "For classifier 10 train accuracy 1.0000 and test accuracy 0.8701\n",
    "For classifier 11 train accuracy 1.0000 and test accuracy 0.8696\n",
    "For classifier 12 train accuracy 0.9947 and test accuracy 0.8268\n",
    "For classifier 13 train accuracy 1.0000 and test accuracy 0.8653\n",
    "For classifier 14 train accuracy 0.9995 and test accuracy 0.8044\n",
    "ENSEMBLE ACCURACY MODE\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "1.0\n",
    "\n",
    "ENSEMBLE ACCURACY MODE\n",
    "0.8979155531801176\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "0.9021913415285944\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "0.8984500267236771\n",
    "\n",
    "big bert [512]\n",
    "\n",
    "For classifier  0 train accuracy 1.0000 and test accuracy 0.8792\n",
    "For classifier  1 train accuracy 1.0000 and test accuracy 0.8787\n",
    "For classifier  2 train accuracy 0.9989 and test accuracy 0.8952\n",
    "For classifier  3 train accuracy 1.0000 and test accuracy 0.8803\n",
    "For classifier  4 train accuracy 1.0000 and test accuracy 0.8632\n",
    "For classifier  5 train accuracy 1.0000 and test accuracy 0.8717\n",
    "For classifier  6 train accuracy 1.0000 and test accuracy 0.8797\n",
    "For classifier  7 train accuracy 1.0000 and test accuracy 0.8733\n",
    "For classifier  8 train accuracy 1.0000 and test accuracy 0.8781\n",
    "For classifier  9 train accuracy 1.0000 and test accuracy 0.8712\n",
    "For classifier 10 train accuracy 1.0000 and test accuracy 0.8616\n",
    "For classifier 11 train accuracy 1.0000 and test accuracy 0.8669\n",
    "For classifier 12 train accuracy 0.9995 and test accuracy 0.8589\n",
    "For classifier 13 train accuracy 1.0000 and test accuracy 0.8632\n",
    "For classifier 14 train accuracy 1.0000 and test accuracy 0.8803\n",
    "ENSEMBLE ACCURACY MODE\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "1.0\n",
    "\n",
    "ENSEMBLE ACCURACY MODE\n",
    "0.9048637092463923\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "0.9075360769641903\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "0.9048637092463923\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "one weight [hidden_size, hidden_size] all between first_sep_token and first_pad_token (15 epochs)\n",
    "\n",
    "For classifier  0 train accuracy 1.0000 and test accuracy 0.8787\n",
    "For classifier  1 train accuracy 1.0000 and test accuracy 0.8899\n",
    "For classifier  2 train accuracy 1.0000 and test accuracy 0.8867\n",
    "For classifier  3 train accuracy 1.0000 and test accuracy 0.8771\n",
    "For classifier  4 train accuracy 0.9995 and test accuracy 0.8904\n",
    "For classifier  5 train accuracy 1.0000 and test accuracy 0.8883\n",
    "For classifier  6 train accuracy 1.0000 and test accuracy 0.8856\n",
    "For classifier  7 train accuracy 1.0000 and test accuracy 0.8840\n",
    "For classifier  8 train accuracy 1.0000 and test accuracy 0.8830\n",
    "For classifier  9 train accuracy 1.0000 and test accuracy 0.8942\n",
    "For classifier 10 train accuracy 1.0000 and test accuracy 0.8985\n",
    "For classifier 11 train accuracy 1.0000 and test accuracy 0.8910\n",
    "For classifier 12 train accuracy 1.0000 and test accuracy 0.8936\n",
    "For classifier 13 train accuracy 1.0000 and test accuracy 0.8936\n",
    "For classifier 14 train accuracy 1.0000 and test accuracy 0.8862\n",
    "ENSEMBLE ACCURACY MODE\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "1.0\n",
    "\n",
    "ENSEMBLE ACCURACY MODE\n",
    "0.9102084446819882\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "0.9064671298770711\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "0.9075360769641903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
