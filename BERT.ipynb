{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/apps/python/3.6.4/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)  # suppress some deprecation warnings\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_directory = './data'\n",
    "\n",
    "# data_train = pd.read_csv(os.path.join(data_directory, 'train_stories.csv'), header='infer')\n",
    "\n",
    "data_val = pd.read_csv(os.path.join(data_directory, 'cloze_test_val__spring2016 - cloze_test_ALL_val.csv'), header='infer')\n",
    "data_test = pd.read_csv(os.path.join(data_directory, 'cloze_test_test__spring2016 - cloze_test_ALL_test.csv'), header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0523 21:10:08.796071 47570318003648 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'output_dir'\n",
    "DO_DELETE = False\n",
    "USE_BUCKET = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /cluster/home/sanagnos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /cluster/home/sanagnos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_val):\n",
    "    new_dataset = []\n",
    "    classes = []\n",
    "    for pos in range(len(data_val)):\n",
    "        story_start = data_val.iloc[pos][['InputSentence' + str(i) for i in [1, 2, 3, 4]]].values\n",
    "        \n",
    "        new_dataset.append(\" \".join(np.append(story_start, \" ||| \" + data_val.iloc[pos]['RandomFifthSentenceQuiz1'])))\n",
    "        new_dataset.append(\" \".join(np.append(story_start, \" ||| \" + data_val.iloc[pos]['RandomFifthSentenceQuiz2'])))\n",
    "        \n",
    "        if data_val.iloc[pos]['AnswerRightEnding'] == 1:\n",
    "            classes.append(0)\n",
    "            classes.append(1)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "            classes.append(0)\n",
    "            \n",
    "    return new_dataset, classes\n",
    "\n",
    "dataset_val, classes_val = create_dataset(data_val)\n",
    "val_pd = pd.DataFrame({'story': dataset_val, 'class': classes_val})\n",
    "\n",
    "dataset_test, classes_test = create_dataset(data_test)\n",
    "test_pd = pd.DataFrame({'story': dataset_test, 'class': classes_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3742\n",
      "3742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# train_size = 0.80\n",
    "\n",
    "# np.random.seed(42)\n",
    "# msk = np.random.rand(len(test_pd) // 2) < train_size\n",
    "# new_mask = np.array([[value, value] for value in msk]).reshape(-1)\n",
    "\n",
    "# train = shuffle(test_pd[new_mask])\n",
    "\n",
    "# test = test_pd[~new_mask]\n",
    "\n",
    "train = shuffle(val_pd)\n",
    "train_unshuffled = val_pd\n",
    "test = test_pd\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'story'\n",
    "LABEL_COLUMN = 'class'\n",
    "# label_list is 0 for a true story and 1 for a false story\n",
    "label_list = [0, 1]\n",
    "\n",
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 96\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "  n_ctx = input_ids.shape[-1].value\n",
    "  \n",
    "#   THIS WORKS PRETTY WELL alsmost 90 ensemble with small bert\n",
    "#   transformer_outputs = bert_outputs['sequence_output']\n",
    "#   output_layer = transformer_outputs[:, -8:]\n",
    "#   print(tf.shape(bert_outputs[\"pooled_output\"][-1]))\n",
    "#   print(output_layer)\n",
    "#   output_layer = tf.concat(output_layer, axis=1)\n",
    "#   print(output_layer)    \n",
    "#   output_layer = tf.reshape(output_layer, [-1, 768 * 8])\n",
    "#   print(output_layer)   \n",
    "\n",
    "    \n",
    "  transformer_outputs = bert_outputs['sequence_output']\n",
    "\n",
    "  weights_size = hidden_size * 5\n",
    "  # These weights are used to multiply all transformer outputs after the separator\n",
    "  output_weights = tf.get_variable(\"output_weights\", [weights_size, hidden_size], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\"output_bias\", [weights_size], initializer=tf.zeros_initializer())\n",
    "  \n",
    "  clf_token = 1064\n",
    "\n",
    "  tf_argmax = tf.argmax(tf.cast(tf.equal(input_ids, clf_token), tf.float32), 1)\n",
    "\n",
    "    \n",
    "  def create_new_state(x):\n",
    "    # remove the 3 separators ... or not ?\n",
    "    i = x[0]\n",
    "    hidden_pos = x[1]\n",
    "    result = tf.zeros([weights_size])\n",
    "    \n",
    "    condition = lambda i, m: i < n_ctx\n",
    "#     body = lambda i, m: [i+1, m + output_weights * hidden_pos[i]]\n",
    "    # \n",
    "    body = lambda i, m: [i+1, m + tf.reshape(tf.matmul(output_weights, tf.expand_dims(hidden_pos[i], 1)), [-1])]\n",
    "\n",
    "\n",
    "\n",
    "    return tf.while_loop(condition, body, loop_vars=[i, result],\n",
    "                         shape_invariants=[i.get_shape(), tf.TensorShape([weights_size])])[1]\n",
    "\n",
    "  output_layer = tf.map_fn(create_new_state, (tf_argmax, transformer_outputs), dtype=tf.float32) + output_bias\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "    logits = tf.layers.dense(output_layer, 512, use_bias=True, activation=tf.nn.sigmoid)\n",
    "    logits = tf.nn.dropout(logits, keep_prob=0.9)\n",
    "    logits = tf.layers.dense(logits, num_labels, use_bias=True)\n",
    "\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "    \n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_steps 3508\n"
     ]
    }
   ],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 15.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 50000\n",
    "SAVE_SUMMARY_STEPS = 100000\n",
    "\n",
    "OUTPUT_DIR = 'output_dir'\n",
    "SAVE_RESULTS_DIR = 'results_predictions'\n",
    "\n",
    "N_ESTIMATORS = 15\n",
    "\n",
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "print('num_train_steps', num_train_steps)\n",
    "\n",
    "# Skip this step to avoid disk quota\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "  num_labels=len(label_list),\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n",
    "\n",
    "\n",
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_predictions(in_sentences):\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    predictions = [prediction['probabilities'] for prediction in predictions]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def combine_predictions(predictions):\n",
    "    my_predictions = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(test):\n",
    "        p_first = np.exp(predictions[i])\n",
    "        p_second = np.exp(predictions[i + 1])\n",
    "\n",
    "        p1 = p_first[0] + p_second[1]\n",
    "        p2 = p_first[1] + p_second[0]\n",
    "\n",
    "        if p1 > p2:\n",
    "            my_predictions.append(1)\n",
    "        else:\n",
    "            my_predictions.append(2)\n",
    "        i += 2\n",
    "        \n",
    "    return np.array(my_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= BEGGINING TRAINING FOR CLASSIFIER  0 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/sanagnos/.local/lib64/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took time  0:39:06.473375\n",
      "Score on train is  1.0\n"
     ]
    }
   ],
   "source": [
    "!rm -rf {SAVE_RESULTS_DIR} || true\n",
    "!mkdir {SAVE_RESULTS_DIR}\n",
    "\n",
    "true_labels_train = train_unshuffled['class'].values[::2] + 1\n",
    "true_labels_val = test['class'].values[::2] + 1\n",
    "\n",
    "for i in range(N_ESTIMATORS):\n",
    "    !rm -rf {OUTPUT_DIR} || true\n",
    "    \n",
    "    train_features = shuffle(train_features)\n",
    "    \n",
    "    # Create an input function for training. drop_remainder = True for using TPUs.\n",
    "    train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "        features=train_features,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=True,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    \n",
    "    print(f'========= BEGGINING TRAINING FOR CLASSIFIER {i:2d} =========')\n",
    "    current_time = datetime.now()\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "    print(\"Training took time \", datetime.now() - current_time)\n",
    "        \n",
    "    predictions = get_final_predictions(train_unshuffled['story'].values)\n",
    "    print('Score on train is ', accuracy_score(true_labels_train, combine_predictions(predictions)))\n",
    "    np.savetxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, \"predictions_train_\" + str(i) + '.csv'), predictions, delimiter=\",\")\n",
    "    \n",
    "    predictions = get_final_predictions(test['story'].values)\n",
    "    val_score = accuracy_score(true_labels_val, combine_predictions(predictions))\n",
    "    print('Score on val is ', val_score)\n",
    "    np.savetxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, \"predictions_test_\" + str(val_score) + '_classifier_' + str(i) + '.csv'), predictions, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For classifier  0 train accuracy 1.0000 and test accuracy 0.8792\n",
      "For classifier  1 train accuracy 1.0000 and test accuracy 0.8787\n",
      "For classifier  2 train accuracy 0.9989 and test accuracy 0.8952\n",
      "For classifier  3 train accuracy 1.0000 and test accuracy 0.8803\n",
      "For classifier  4 train accuracy 1.0000 and test accuracy 0.8632\n",
      "For classifier  5 train accuracy 1.0000 and test accuracy 0.8717\n",
      "For classifier  6 train accuracy 1.0000 and test accuracy 0.8797\n",
      "For classifier  7 train accuracy 1.0000 and test accuracy 0.8733\n",
      "For classifier  8 train accuracy 1.0000 and test accuracy 0.8781\n",
      "For classifier  9 train accuracy 1.0000 and test accuracy 0.8712\n",
      "For classifier 10 train accuracy 1.0000 and test accuracy 0.8616\n",
      "For classifier 11 train accuracy 1.0000 and test accuracy 0.8669\n",
      "For classifier 12 train accuracy 0.9995 and test accuracy 0.8589\n",
      "For classifier 13 train accuracy 1.0000 and test accuracy 0.8632\n",
      "For classifier 14 train accuracy 1.0000 and test accuracy 0.8803\n",
      "ENSEMBLE ACCURACY MODE\n",
      "1.0\n",
      "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
      "1.0\n",
      "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
      "1.0\n",
      "\n",
      "ENSEMBLE ACCURACY MODE\n",
      "0.9048637092463923\n",
      "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
      "0.9075360769641903\n",
      "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
      "0.9048637092463923\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "SAVE_RESULTS_DIR = 'results_predictions'\n",
    "\n",
    "\n",
    "files = [f for f in listdir(SAVE_RESULTS_DIR) if isfile(join(SAVE_RESULTS_DIR, f))]\n",
    "\n",
    "true_labels_train = train_unshuffled['class'].values[::2] + 1\n",
    "true_labels_test = test['class'].values[::2] + 1\n",
    "\n",
    "classifiers = [int(file.split(\"_\")[2].split(\".\")[0]) for file in files]\n",
    "num_classifiers = np.max(classifiers)\n",
    "\n",
    "predictions_train = []\n",
    "predictions_test = []\n",
    "for i in range(num_classifiers + 1):\n",
    "    predictions_file_train = np.genfromtxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, \n",
    "                                                        \"predictions_train_\" + str(i) + '.csv'), delimiter=',')\n",
    "    predictions_train.append(predictions_file_train)\n",
    "    \n",
    "    predictions_file_test = np.genfromtxt(os.path.join(\"./\" + SAVE_RESULTS_DIR, \n",
    "                                                       \"predictions_test_\" + str(i) + '.csv'), delimiter=',')\n",
    "    predictions_test.append(predictions_file_test)\n",
    "    print(f'For classifier {i:2d} train accuracy {accuracy_score(true_labels_train, combine_predictions(predictions_file_train)):.4f} and test accuracy {accuracy_score(true_labels_test, combine_predictions(predictions_file_test)):.4f}')\n",
    "    \n",
    "def print_ensemble_predictions(predictions, true_labels):\n",
    "    preds_mode = [combine_predictions(p) for p in predictions]\n",
    "    preds_mode = np.array(preds_mode)\n",
    "    preds_mode = stats.mode(preds_mode)[0][0]\n",
    "\n",
    "    print('ENSEMBLE ACCURACY MODE')\n",
    "    print(accuracy_score(true_labels, preds_mode))\n",
    "\n",
    "    preds_prob = np.mean(predictions, axis=0)\n",
    "    preds_prob = combine_predictions(preds_prob)\n",
    "\n",
    "    print('ENSEMBLE ACCURACY PROB MEAN ON LOGS')\n",
    "    print(accuracy_score(true_labels, preds_prob))\n",
    "\n",
    "\n",
    "    preds_prob = np.log(np.mean(np.exp(predictions), axis=0))\n",
    "    preds_prob = combine_predictions(preds_prob)\n",
    "\n",
    "    print('ENSEMBLE ACCURACY PROB MEAN ON PROBS')\n",
    "    print(accuracy_score(true_labels, preds_prob))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "print_ensemble_predictions(predictions_train, true_labels_train)\n",
    "print_ensemble_predictions(predictions_test, true_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85567284, -0.55338029],\n",
       "       [-0.87034223, -0.54267442],\n",
       "       [-0.60520196, -0.78957908],\n",
       "       ...,\n",
       "       [-1.29842089, -0.31877721],\n",
       "       [-1.00223294, -0.4573779 ],\n",
       "       [-0.17982265, -1.80434856]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_bert (whole val and whole test)\n",
    "\n",
    "output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "logits = tf.layers.dense(output_layer, 128, use_bias=True, activation=tf.nn.sigmoid)\n",
    "logits = tf.nn.dropout(logits, keep_prob=0.9)\n",
    "logits = tf.layers.dense(logits, num_labels, use_bias=True)\n",
    "\n",
    "For classifier  0 train accuracy 1.0000 and test accuracy 0.8391\n",
    "For classifier  1 train accuracy 1.0000 and test accuracy 0.8343\n",
    "For classifier  2 train accuracy 1.0000 and test accuracy 0.8386\n",
    "For classifier  3 train accuracy 1.0000 and test accuracy 0.8311\n",
    "For classifier  4 train accuracy 1.0000 and test accuracy 0.8434\n",
    "For classifier  5 train accuracy 1.0000 and test accuracy 0.8397\n",
    "For classifier  6 train accuracy 1.0000 and test accuracy 0.8434\n",
    "For classifier  7 train accuracy 1.0000 and test accuracy 0.8418\n",
    "For classifier  8 train accuracy 1.0000 and test accuracy 0.8359\n",
    "For classifier  9 train accuracy 1.0000 and test accuracy 0.8487\n",
    "For classifier 10 train accuracy 1.0000 and test accuracy 0.8332\n",
    "For classifier 11 train accuracy 1.0000 and test accuracy 0.8429\n",
    "For classifier 12 train accuracy 1.0000 and test accuracy 0.8397\n",
    "For classifier 13 train accuracy 1.0000 and test accuracy 0.8407\n",
    "For classifier 14 train accuracy 1.0000 and test accuracy 0.8365\n",
    "ENSEMBLE ACCURACY MODE\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "1.0\n",
    "\n",
    "ENSEMBLE ACCURACY MODE\n",
    "0.8722608230892571\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "0.8786745056119722\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "0.8786745056119722\n",
    "\n",
    "big_bert (whole val and whole test)\n",
    "For classifier  0 train accuracy 1.0000 and test accuracy 0.8691\n",
    "For classifier  1 train accuracy 1.0000 and test accuracy 0.8664\n",
    "For classifier  2 train accuracy 1.0000 and test accuracy 0.8680\n",
    "For classifier  3 train accuracy 1.0000 and test accuracy 0.8562\n",
    "For classifier  4 train accuracy 0.9984 and test accuracy 0.8397\n",
    "For classifier  5 train accuracy 1.0000 and test accuracy 0.8696\n",
    "For classifier  6 train accuracy 1.0000 and test accuracy 0.8381\n",
    "For classifier  7 train accuracy 1.0000 and test accuracy 0.8589\n",
    "For classifier  8 train accuracy 1.0000 and test accuracy 0.8632\n",
    "For classifier  9 train accuracy 1.0000 and test accuracy 0.8600\n",
    "For classifier 10 train accuracy 1.0000 and test accuracy 0.8701\n",
    "For classifier 11 train accuracy 1.0000 and test accuracy 0.8696\n",
    "For classifier 12 train accuracy 0.9947 and test accuracy 0.8268\n",
    "For classifier 13 train accuracy 1.0000 and test accuracy 0.8653\n",
    "For classifier 14 train accuracy 0.9995 and test accuracy 0.8044\n",
    "ENSEMBLE ACCURACY MODE\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "1.0\n",
    "\n",
    "ENSEMBLE ACCURACY MODE\n",
    "0.8979155531801176\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "0.9021913415285944\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "0.8984500267236771\n",
    "\n",
    "big bert [512]\n",
    "\n",
    "For classifier  0 train accuracy 1.0000 and test accuracy 0.8792\n",
    "For classifier  1 train accuracy 1.0000 and test accuracy 0.8787\n",
    "For classifier  2 train accuracy 0.9989 and test accuracy 0.8952\n",
    "For classifier  3 train accuracy 1.0000 and test accuracy 0.8803\n",
    "For classifier  4 train accuracy 1.0000 and test accuracy 0.8632\n",
    "For classifier  5 train accuracy 1.0000 and test accuracy 0.8717\n",
    "For classifier  6 train accuracy 1.0000 and test accuracy 0.8797\n",
    "For classifier  7 train accuracy 1.0000 and test accuracy 0.8733\n",
    "For classifier  8 train accuracy 1.0000 and test accuracy 0.8781\n",
    "For classifier  9 train accuracy 1.0000 and test accuracy 0.8712\n",
    "For classifier 10 train accuracy 1.0000 and test accuracy 0.8616\n",
    "For classifier 11 train accuracy 1.0000 and test accuracy 0.8669\n",
    "For classifier 12 train accuracy 0.9995 and test accuracy 0.8589\n",
    "For classifier 13 train accuracy 1.0000 and test accuracy 0.8632\n",
    "For classifier 14 train accuracy 1.0000 and test accuracy 0.8803\n",
    "ENSEMBLE ACCURACY MODE\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "1.0\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "1.0\n",
    "\n",
    "ENSEMBLE ACCURACY MODE\n",
    "0.9048637092463923\n",
    "ENSEMBLE ACCURACY PROB MEAN ON LOGS\n",
    "0.9075360769641903\n",
    "ENSEMBLE ACCURACY PROB MEAN ON PROBS\n",
    "0.9048637092463923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
